<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/css/font-awesome.min.css"> <link rel=stylesheet  href="/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=192x192  href="/assets/favicon.png"> <link rel="shortcut icon" href="/assets/favicon.ico"> <link rel=apple-touch-icon-precomposed  sizes=152x152  href="/assets/apple-touch-icon.png"> <title>Post 2</title> <header> <nav class=nav-main > <ul> <li class=hvr-underline-reveal ><a href="/">Home</a> <li class=hvr-underline-reveal ><a href="/research/">Research</a> <li class=logo ><a class=hvr-ripple-out  href="/">DI</a> <li class=hvr-underline-reveal ><a href="/presentations/">Presentations</a> <li class=hvr-underline-reveal ><a href="/blog/">Blog</a> </ul> </nav> </header> <div class=franklin-content > <h1 id=probability_of_constraint_satisfaction_and_robust_optimization ><a href="#probability_of_constraint_satisfaction_and_robust_optimization" class=header-anchor >Probability of constraint satisfaction and Robust Optimization</a></h1> <p>This post explores some of the properties of robust mean-variance optimization. </p> <p>Consider the classic mean-variance optimization model to select a portfolio \(\bf{x} \in \mathbb{R}^N\)</p> \[ \min_{\bf{x}} \bf{x}^{\intercal} Q \bf{x} \quad \textit{s.t.}\quad \bf{1}^{\intercal}\bf{x} = 1, \ \bf{x} \geq \bf{0},\ \boldsymbol{\mu}^{\intercal}\bf{x} \geq r_{\text{min}}, \] <p>where \(r_{\text{min}}\) is the target return, \(\bf{Q}\) is the covariance matrix of asset returns, and \(\mathbf{\mu}\) is the expected return. </p> <p>We do not know the true values of \(\boldsymbol{\mu}\) and \(\bf{Q}\). Instead, we only have data to estimate these quantities. We form the corresponding estimates \(\hat{\boldsymbol{\mu}}\) and \(\hat{\bf{Q}}\) from a dataset of returns \(S = \{\mathbf{r}^{(i)}\}_{i=1}^T\) and solve the MVO model with \(\boldsymbol{\mu}\) and \(\bf{Q}\) replaced with \(\hat{\boldsymbol{\mu}}\) and \(\hat{\bf{Q}}\). Let \({\bf x}_{\text{MVO}} \left(\boldsymbol{\hat{\mu}}, \hat{\bf{Q}} \right)\) denote an MVO solution obtained using estimates \(\boldsymbol{\hat{\mu}}\) and \(\hat{\bf{Q}}\).</p> <p>What is random here? The estimates \(\boldsymbol{\hat{\mu}}\) and \(\hat{\bf{Q}}\) are random, since they depend on a random sample \(S\). We can then ask, &quot;What is the probability of meeting the constraints?&quot;</p> <p>\(\mathbb{P}_S[\mu^{\intercal} {\bf x}_{\text{MVO}} \left(\boldsymbol{\mu}, \bf{Q} \right) \geq r_{\text{min}}] = 1\) since \({\bf x}_{\text{MVO}}\) is a deterministic quantity and satisfies the return constraint by definition. </p> <p>Similarly, one can consider the probability of meeting the return constraint when using the estimates: \(\mathbb{P}[\mu^{\intercal} {\bf x}_{\text{MVO}} \left(\boldsymbol{\hat{\mu}}, \hat{\bf{Q}} \right) \geq r_{\text{min}}] = ? \). Unfortunately, this probability does not equal 1. This drawback motivates the use of robust optimization, where we replace the constraint</p> \[ \boldsymbol{\mu}^{\intercal}\bf{x} \geq r_{\text{min}} \] <p>with</p> \[ \boldsymbol{\mu}^{\intercal}\bf{x} \geq r_{\text{min}} \ \forall \boldsymbol{\mu} \in \mathcal{U}, \] <p>where \(\mathcal{U}\) is an uncertainty set for the mean. A popular uncertainty set is the ellipsoid centered at the estimate \(\hat\boldsymbol{\mu}\) with shape parameter </p> \[\Theta = \frac{1}{T}\begin{pmatrix} \hat{Q}_{11} & 0 & \cdots & 0 \\ 0 & \hat{Q}_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \hat{Q}_{nn} \end{pmatrix} \] <p>and radius \(\delta\). In this setting, the robust optimization problem can be written as: </p> \[ \begin{align*} &\begin{align*} & \min_{\bf{x}} & \bf{x}^{\intercal} \hat{\bf{Q}}\ \bf{x} \end{align*}\\ &\begin{align*} &\ \mathrm{s.t.} & \hat{\boldsymbol{\mu}}^T \bf{x} - \delta \|\bf{\Theta}^{1/2} \bf{x}\|_2 &\geq r_{\text{min}} \\ & & \bf{1}^T \bf{x} &= 1 \end{align*} \end{align*} \] <p>Now let \({\bf x}_{\text{ROB}} \left(\boldsymbol{\hat{\mu}}, \hat{\bf{Q}} \right)\) denote a robust MVO solution obtained using estimates \(\boldsymbol{\hat{\mu}}\) and \(\hat{\bf{Q}}\) and consider the probability of constraint satisfaction \(\mathbb{P}[\mu^{\intercal} {\bf x}_{\text{ROB}} \left(\boldsymbol{\hat{\mu}}, \hat{\bf{Q}} \right) \geq r_{\text{min}}] = ? \). This probability should be higher than the estimated MVO case if the ellipsoid covers a large portion of the support of the distribution of \(\hat{\boldsymbol{\mu}}\). However, in practice, we cannot easily evaluate these probabilities. </p> <h2 id=google_colab_notebook ><a href="#google_colab_notebook" class=header-anchor >Google Colab Notebook</a></h2> <p>The notebook located <a href="https://colab.research.google.com/drive/1miJ8VW_YhU3dDYCLTiv61soPiyNXGSG0?usp&#61;sharing">here</a> explores these facts in a contrived setting where we have sampling access to the return distribution. Indeed, the robust model increases the probability of satisfying the return constraint. </p> <div class=page-foot > <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> David Islip. Last modified: May 27, 2025. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/contrib/auto-render.min.js"></script> <script>renderMathInElement(document.body)</script>